# Refer to https://github.com/terrytangyuan/distributed-ml-patterns/blob/main/code/project/code/inference-service.yaml
# The inference service using tensorflow model format will utilize tensorflow/serving docker image
apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: {{ .Values.inferenceService.name }}
spec:
  predictor:
    resources:
      limits:
        nvidia.com/gpu: {{ .Values.resources.limits.gpu }}
    model:
      modelFormat:
        name: {{ .Values.inferenceService.modelFormat.name }}
      storageUri: {{ .Values.inferenceService.storageUri }}
      serviceAccountName: {{ .Values.serviceAccount.name }}
