# Refer to https://github.com/terrytangyuan/distributed-ml-patterns/blob/main/code/project/code/inference-service.yaml
# The inference service using tensorflow model format will utilize tensorflow/serving docker image
apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: {{ .Values.inferenceService.name }}
spec:
  predictor:
    # resources:
    #   requests:
    #     cpu: {{ .Values.resources.requests.cpu }}
    #     memory: {{ .Values.resources.requests.memory }}
    #   limits:
    #     cpu: {{ .Values.resources.limits.cpu }}
    #     memory: {{ .Values.resources.limits.memory }}
    model:
      modelFormat:
        name: {{ .Values.inferenceService.modelFormat.name }}
      storageUri: {{ .Values.inferenceService.storageUri }}
      serviceAccountName: {{ .Values.serviceAccount.name }}
